{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyNBS import data_import_tools as dit\n",
    "from pyNBS import network_propagation as prop\n",
    "from pyNBS import pyNBS_core as core\n",
    "from pyNBS import pyNBS_single\n",
    "from pyNBS import consensus_clustering as cc\n",
    "from pyNBS import pyNBS_plotting as plot\n",
    "\n",
    "import time\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load non-default parameters\n",
    "Notes about the parameter file:\n",
    " - If no parameter path is given, default parameters will be set instead (see documentation for details and default values). \n",
    " - The parameter file is a 2-column comma-separated text file where the first column is the parameter name, and the second column is the parameter value. The delimiter for this file must be a comma. \n",
    " - Blank lines and lines starting with \"#\" will be ignored.\n",
    " - The parameter file may include as many or as few of the parameters from the pyNBS overall parameter space. For examples of two parameter files see: ```./OV_run_pyNBS_Hofree_params.csv``` VS ```./run_pyNBS_default_params.csv```\n",
    " \n",
    " \n",
    "An excerpt of the the default parameters file is given below:  \n",
    "```\n",
    "################################  \n",
    "#   Overall pyNBS parameters   #  \n",
    "################################  \n",
    "verbose,True  \n",
    "outdir,./Results/  \n",
    "  \n",
    "###############################  \n",
    "#   Data Loading Parameters   #  \n",
    "###############################  \n",
    "net_filedelim,\"\t\"  \n",
    "mut_filetype,matrix  \n",
    "mut_filedelim,\",\"  \n",
    "degree_preserved_shuffle,False  \n",
    "node_label_shuffle,False  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyNBS_params = dit.load_params(params_file='./OV_run_pyNBS_Hofree_params.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load molecular network\n",
    "The network file is a 2-column text file representing an unweighted network. Each row represents a single edge in the molecular network.    \n",
    "  \n",
    "Notes about the network file:  \n",
    " - The default column delimiter is a tab character '\\t' but a different delimiter can be defined by the user here or in the parameter file with the \"net_filedelim\" parameter.\n",
    " - The network must not contain duplicate edges (e.g. TP53\\tMDM2 is equivalent to MDM2\\tTP53)\n",
    " - The network must not contain self-edges (e.g. TP53\\tTP53)\n",
    " - Only the first two columns of a network file are read as edges for the network, all other columns will be ignored.\n",
    " - The load_network function also includes options to read in edge- or label-shuffled versions of the network, but by default, these options are turned off.\n",
    " \n",
    "An excerpt of the first five rows of the PID network file is given below:  \n",
    "```\n",
    "A1BG\tA2M\n",
    "A1BG\tAKT1\n",
    "A1BG\tGRB2\n",
    "A1BG\tPIK3CA\n",
    "A1BG\tPIK3R1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network File Loaded: ./Example_Notebook_Data/Network_Files/HM90.sif\n"
     ]
    }
   ],
   "source": [
    "# The only required parameter here is the network file path, it may be defined by the user in the parameter file\n",
    "# or explicitly here.\n",
    "network_filepath = pyNBS_params['network_file']\n",
    "network = dit.load_network_file(network_filepath, params=pyNBS_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load binary somatic mutation data\n",
    "The binary somatic mutation data file can be represented in two file formats:  \n",
    "The ```matrix``` binary somatic mutation data format. This file format is a binary csv or tsv matrix with rows represent samples/patients and columns represent genes.  The following table is a small excerpt of a matrix somatic mutation data file:  \n",
    "\n",
    "||A1CF|A2BP1|A2M|\n",
    "|-|-|-|-|\n",
    "|TCGA-04-1638|0|0|1|\n",
    "|TCGA-23-1029|1|0|0|\n",
    "|TCGA-23-2647|0|1|0|\n",
    "|TCGA-24-1847|0|0|1|\n",
    "|TCGA-42-2589|1|0|0|\n",
    "\n",
    "The ```list``` binary somatic mutation data format. This file format is a 2-column csv or tsv list where the 1st column is a sample/patient and the 2nd column is a gene mutated in the sample/patient. There are no headers in this file format. Loading data with the list format is typically faster than loading data from the matrix format.The following text is the list representation of the matrix above.\n",
    "```\n",
    "TCGA-04-1638\tA2M\n",
    "TCGA-23-1029\tA1CF\n",
    "TCGA-23-2647\tA2BP1\n",
    "TCGA-24-1847\tA2M\n",
    "TCGA-42-2589\tA1CF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Mutation Matrix Loaded: ./Example_Notebook_Data/Mutation_Files/OV_sm_mat_Hofree.csv\n"
     ]
    }
   ],
   "source": [
    "# The only required parameter here is the somatic mutation data file path, it may be defined by the user in the \n",
    "# parameter file or explicitly here.\n",
    "sm_data_filepath = pyNBS_params['sm_data_file']\n",
    "sm_mat = dit.load_binary_mutation_data(sm_data_filepath, params=pyNBS_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Construct regularization graph for use in network-regularized NMF\n",
    "In this step, we will construct the graph used in the network-regularized non-negative matrix factorization (netNMF) step of pyNBS. This network is a K-nearest neighbor (KNN) network constructed from the network influence matrix (Vandin et al 2011*) of the molecular network being used to stratify tumor samples. The graph laplacian of this KNN network (knnGlap) is used as the regularizer in the following netNMF steps. This step uses the `network_inf_KNN_glap()` function in the pyNBS_core module. The function constructs the knnGlap with the following steps:  \n",
    "  \n",
    "__Steps to construct knnGlap:__\n",
    "1. Construct the network influence matrix as described by Vandin et al 2011*\n",
    "> i. Construct laplacian matrix of the molecular network.  \n",
    "> ii. Adjust diagonal of the laplacian matrix by small gamma factor (default 0.01)  \n",
    "> iii. Calculate the inverse of the diagonal-adjusted graph laplacian from (ii) to get the network influence matrix <br> Note: _This method is significantly faster and gives similar results as the original method used previously in Hofree's NBS v0.2.0, which calculated the pseudoinverse of each network component._\n",
    "2. Construct KNN graph by conneting each node to its k nearest neighbors by influence score (default k is 11)\n",
    "3. Calculate graph laplacian of this new KNN graph and return it as knnGlap\n",
    "\n",
    "---\n",
    "* Fabio Vandin, Eli Upfal, and Benjamin J. Raphael. Journal of Computational Biology. March 2011, 18(3): 507-522. https://doi.org/10.1089/cmb.2010.0265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Constructing knnGlap\n",
    "knnGlap = core.network_inf_KNN_glap(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct network propagation kernel matrix\n",
    "Due to the multiple subsampling and propagation steps used in pyNBS, we have found that the algorithm can be significantly sped up for large numbers of subsampling and propagation iterations if a network propagation kernel can be pre-computed. Here we compute this propagation kernel by propagating the all genes in the molecular network independently of one another. The propagation profile of each tumor is then simply the row sums of all genes marked as mutated in each tumor, rather than having to perform the full network propagation step again after each subsampling of the data. Re-propagating at each subsampling step can be time consuming due to the matrix inversion calculation required by our implementation of the network propagation algorithm, which is based on the closed form of the random walk model over networks presented by the HotNet2 paper (Leiserson et al 2015*). This step uses the `network_propagation()` function in the network_propagation module of pyNBS.<br>\n",
    "\n",
    "The general formulation of the closed form random-walk propagation for a given network: <br>\n",
    "$$F_t = (1-\\alpha)*F_0*(I-\\alpha*A)^{-1}$$<br>\n",
    "$F_0$ here is typically the binary mutation matrix, but for the kernel, it is the same as the identity matrix. A is the normalized adjcency matrix of the molecular network. The normalized adjacency matrix is calculated as $A*D^{-1}$ where D is the row/column-matched diagonalized node degree of the molecular network. $\\alpha$ is the network propagation constant, a default value of 0.7 is used here (same as by Hofree), but results may vary if $\\alpha$ is changed. Analysis by Hofree et al suggest that setting $\\alpha$ between 0.5-0.8 gives relatively stable results.\n",
    "  \n",
    "Additional notes about how we perform random-walk based network propagation:  \n",
    " - We first separate the molecular network into each connected component and then perform network propagation for each connected component and concatenate the resulting kernel matrices along the diagonal for each subgraph.\n",
    " - Our default normalized adjacency matrix is not symmetric, but an option is available to calculate a symmetric degree-normalized adjacency matrix defined as $D^{-0.5} * A * D^{-0.5}$.  \n",
    " \n",
    "---\n",
    "* Leiserson MDM, Vandin F, Wu H-T, et al. Pan-Cancer Network Analysis Identifies Combinations of Rare Somatic Mutations across Pathways and Protein Complexes. Nature genetics. 2015;47(2):106-114. doi:10.1038/ng.3168."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing network propagation with alpha: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Calculate propagation kernel by propagating identity matrix of network\n",
    "network_nodes = network.nodes()\n",
    "network_I = pd.DataFrame(np.identity(len(network_nodes)), index=network_nodes, columns=network_nodes)\n",
    "kernel = prop.network_propagation(network, network_I, params=pyNBS_params)\n",
    "print 'Network kernel calculated'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsampling, propagation, and netNMF\n",
    "After the pre-computation of the regularization graph laplacian and the network propagation kernel, we perform the core steps of the NBS algorithm multiple times (100x here) to produce multiple patient clusters that will be used in the later consensus clustering step. Each patient clustering is performed with the following steps:  \n",
    "  \n",
    "__1. Subsample binary somatic mutation data__\n",
    ">  i. Reduce binary somatic mutation data matrix to only contain columns of genes found in the network.  \n",
    ">  ii. Sub-sample rows (samples/tumors) and columns (network genes) of the binary somatic mutation matrix. Default is 80% of each axis. This can be changed with the *pats\\_subsample\\_p* and *gene\\_subsample\\_p* value in the parameter file/dictionary.  \n",
    ">  iii. Filter all rows with less than the minimum number of mutations. Default is 10 mutations. This can be changed with the *min\\_muts* value in the parameter file/dictionary.  \n",
    "  \n",
    "__2. Propagate binary somatic mutation data over network__\n",
    ">  i. If no network propagation kernel is pre-computed/provided, use the closed form of the random walk model over the full network by subgraphs (HotNet2)  \n",
    "> ii. If a pre-calculated kernel is provided, calculate individual propagation profiles by calculating the column sums of all genes mutated in the patient for each patient. This method saves a significant amount of time when performing many iterations of these steps.  \n",
    "  \n",
    "__3. Quantile normalize the network-smoothed mutation data__\n",
    ">  i. Sort each patient by gene propagation value  \n",
    ">  ii. Rank averages for each gene across samples  \n",
    ">  iii. Assign ranked averages to ranks of each gene for each sample  \n",
    "  \n",
    "__4. Use netNMF to decompose network data into k clusters__\n",
    ">  i. In each iteration, update W and H with network constraints until W and H converge  \n",
    "> ii. Track reconstruction errors and residuals  \n",
    "  \n",
    "These steps are wrapped by the `pyNBS_single()` function, which calls each step above as a different function from the `pyNBS_core module` (with the exception of the network propagation step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Hlist = []\n",
    "for i in range(pyNBS_params['niter']):\n",
    "    netNMF_time = time.time()\n",
    "    pyNBS_params['pyNBS_iteration'] = repr(i+1)\n",
    "    pyNBS_params['verbose'] = False # Turn off reporting for now at this stage\n",
    "    Hlist.append(pyNBS_single.NBS_single(sm_mat, propNet=network, propNet_kernel=kernel, regNet_glap=knnGlap, params=pyNBS_params))\n",
    "    if pyNBS_params['verbose']:\n",
    "        t = time.time()-netNMF_time\n",
    "        print 'NBS iteration:', i+1, 'complete:', t, 'seconds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consensus Clustering\n",
    "In order to produce robust patient clusters, the sub-sampling and re-clustering steps as done above are needed. After the patient data is subsampled multiple times (default=100), we perform the following step on each individual clustering result to create a single consensus clustering of all patients.  \n",
    "  \n",
    "__Steps for consensus clustering__\n",
    "1. Assign each patient to a cluster by the argmax column in the 'H' matrix for each netNMF result (termed 'hard cluster assignment')\n",
    "2. Construct patient x patient co-clustering matrix where each element is the proportion of all clustering iterations where any two patients were assigned to the same cluster.\n",
    "3. Transform this simiarity matrix into a distance matrix by taking 1-[co_clustering_matrix]\n",
    "4. Apply hierarchical clustering to the resulting distance matrix from (3) by average linkage and cut the hierarchy at desired depth k (the number of clusters, default is 3) to assign patients to a consensus cluster.\n",
    "\n",
    "This step uses the consensus_hclust_hard() function in the conensus_clustering module. It accepts a list of pandas dataframes as generated in the previous step. If the H matrices were generated separately and saved to a directory, the user will need to manually import those H matrices into a python list first before passing the list to the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# ---------- NBS Consensus Clustering Functions ---------- #\n",
    "############################################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.cluster.hierarchy as hclust\n",
    "\n",
    "# Constructs Hlist object for consensus clustering functions if NBS iterations were run in parallel and outputs saved to a folder\n",
    "def Hlist_constructor_from_folder(folder, ext='.csv', normalize_H=False, verbose=False):\n",
    "    co_clustering_results = [folder+fn for fn in os.listdir(folder) if fn.endswith(ext)]\n",
    "    # Generate list of patient clusterings from netNMF\n",
    "    Hlist = [pd.read_csv(fn, index_col=0) for fn in co_clustering_results]\n",
    "    # Normalize H matrices if needed (to make columns comparable if not already done in decomposition)\n",
    "    if normalize_H:\n",
    "        Hlist_norm = []\n",
    "        for H in Hlist:\n",
    "            H_norm = np.dot(H,np.diag(1/H.sum()))\n",
    "            Hlist_norm.append(pd.DataFrame(H_norm, index=H.index))\n",
    "        if verbose:\n",
    "            print 'Hlist constructed and normalized'\n",
    "        return Hlist_norm\n",
    "    else:\n",
    "        if verbose:\n",
    "            print 'Hlist constructed'\n",
    "        return Hlist\n",
    "\n",
    "# Takes a list of 'H' (patient-by-k) dataframes and performs 'hard' consensus clustering\n",
    "# Using hierarchical clustering and average linkage\n",
    "# Returns similarity table (distance is 1-similarity) and linkage map of patients\n",
    "# Also returns cluster assignment map of patients if wanted\n",
    "def consensus_hclust_hard(Hlist, params=None):\n",
    "    # Load and set consensus clustering parameters\n",
    "    # Make sure all H matrices are pandas DataFrames\n",
    "    if not all([type(H)==pd.DataFrame for H in Hlist]):\n",
    "        raise ValueError('Not all H matrices given are pandas DataFrames.')\n",
    "    k = Hlist[0].shape[1]\n",
    "    hclust_linkage_method = 'average'\n",
    "    hclust_linkage_metric = 'euclidian'\n",
    "    save_cc_matrix = True\n",
    "    save_clusters = True\n",
    "    verbose = False\n",
    "    if (params is not None) and (type(params)==dict):\n",
    "        if 'netNMF_k' in params:\n",
    "            k = int(params['netNMF_k'])         \n",
    "        if 'hclust_linkage_method' in params:\n",
    "            hclust_linkage_method = str(params['hclust_linkage_method'])   \n",
    "        if 'hclust_linkage_metric' in params:\n",
    "            hclust_linkage_metric = str(params['hclust_linkage_metric'])   \n",
    "        if 'save_cc_matrix' in params:\n",
    "            save_cc_matrix = bool(params['save_cc_matrix'])   \n",
    "        if 'save_clusters' in params:\n",
    "            save_clusters = bool(params['save_clusters'])   \n",
    "        if 'verbose' in params:\n",
    "            verbose = bool(params['verbose']) \n",
    "    # Generate patient list\n",
    "    pat_list = set()\n",
    "    for H in Hlist:\n",
    "        pat_list = pat_list.union(set(H.index))\n",
    "    pat_list = sorted(list(pat_list))\n",
    "    if verbose:\n",
    "        print 'Constructing Hlist:', len(Hlist), 'cluster matrices', len(pat_list), 'samples'\n",
    "\n",
    "    # Initialzie co-clustering tables\n",
    "    co_clust_table = pd.DataFrame(0, index=pat_list, columns=pat_list)\n",
    "    cluster_count = pd.DataFrame(0, index=pat_list, columns=pat_list)\n",
    "\n",
    "    # Calculate patient similarities and linkage\n",
    "    for H in Hlist:\n",
    "        H.columns = range(1,len(H.columns)+1)\n",
    "        # Update patient cluster count\n",
    "        cluster_count.ix[H.index, H.index]+=1\n",
    "        # Get cluster assignment for each patient\n",
    "        cluster_assign = {i:[] for i in H.columns}\n",
    "        for pat in H.index:\n",
    "            cluster_assign[np.argmax(H.ix[pat])].append(pat)\n",
    "        # Update co-clustering matrix with each cluster assignment\n",
    "        for cluster in cluster_assign:\n",
    "            cluster_pats = cluster_assign[cluster]\n",
    "            co_clust_table.ix[cluster_pats, cluster_pats]+=1\n",
    "    cc_hard_sim_table = co_clust_table.astype(float).divide(cluster_count.astype(float)).fillna(0)\n",
    "    cc_hard_dist_table = 1-cc_hard_sim_table\n",
    "    Z = hclust.linkage(dist.squareform(np.array(cc_hard_dist_table)), method=hclust_linkage_method, metric=hclust_linkage_metric)\n",
    "    cluster_map = hclust.fcluster(Z, k, criterion='maxclust')\n",
    "    cluster_assign = pd.Series({cc_hard_dist_table.index[i]:cluster_map[i] for i in range(len(cc_hard_dist_table.index))}, name='CC Hard, k='+repr(k))\n",
    "    if save_cc_matrix:\n",
    "        save_cc_matrix_path = params['outdir']+params['job_name']+'_cc_matrix.csv'\n",
    "        cc_hard_sim_table.to_csv(save_cc_matrix_path)\n",
    "    if save_clusters:\n",
    "        save_clusters_path = params['outdir']+params['job_name']+'_cluster_assignments.csv'\n",
    "        cluster_assign.to_csv(save_clusters_path)        \n",
    "    if verbose:\n",
    "        print 'Hlist consensus constructed and sample clusters assigned'\n",
    "    return cc_hard_sim_table, Z, cluster_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pyNBS_params['verbose'] = True\n",
    "NBS_cc_table, NBS_cc_linkage, NBS_cluster_assign = consensus_hclust_hard(Hlist, params=pyNBS_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co-Clustering Map\n",
    "To visualize the clusters formed by the pyNBS algorithm, we can plot a similarity map using the objects created in the previous step. We will also load data from the original Hofree et al 2013 paper to compare the results of the pyNBS implementation of the algorithm to the results reported in the paper. This step uses the `cluster_color_assign()` and `plot_cc_map()` functions in the `pyNBS_plotting` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First load the cluster assignment data from Hofree 2013 for ovarian cancer patients\n",
    "orig_Hofree_OV_clust = pd.read_table('./Example_Notebook_Data/Hofree_Results/Hofree_OV_NBS_Results.csv',sep=',',index_col=0)\n",
    "\n",
    "# Align the pyNBS and Hofree cluster assignments with one another\n",
    "cluster_align = pd.concat([orig_Hofree_OV_clust.iloc[:,0], NBS_cluster_assign], axis=1).dropna(axis=0,how='any').astype(int)\n",
    "Hofree_OV_clust = cluster_align.iloc[:,0].astype(int)\n",
    "pyNBS_OV_clust = cluster_align.iloc[:,1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Assign colors to clusters from Hofree and pyNBS\n",
    "Hofree_OV_clust_cmap = plot.cluster_color_assign(Hofree_OV_clust, name='Hofree OV Cluster Assignments')\n",
    "pyNBS_OV_clust_cmap = plot.cluster_color_assign(pyNBS_OV_clust, name='pyNBS OV Cluster Assignments')\n",
    "# Plot and save co-cluster map figure\n",
    "plot.plot_cc_map(NBS_cc_table, NBS_cc_linkage, row_color_map=Hofree_OV_clust_cmap, col_color_map=pyNBS_OV_clust_cmap, \n",
    "                 params=pyNBS_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename = pyNBS_params['outdir']+pyNBS_params['job_name']+'_cc_map.png', width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Survival analysis\n",
    "To determine if the patient clusters are prognostically relevant, we perform a standard survival analysis using a multi-class logrank test to evaluate the significance of survival separation between patient clusters. This data is plotted using a Kaplan-Meier plot using the `cluster_KMplot()` in the `pyNBS_plotting` module. However in order to plot the survival differences between clusters, we will need to load survival data for each patient. This data was extracted from TCGA clinical data. The survival data is given in a 4-column delimited table with the specific headings described below (the columns must be in the same order as shown below). \n",
    "  \n",
    "__Survival Table Column Headers:__\n",
    " - `vital_status`: Binary (0/1) value of whether or not the patient is alive (censored). 0 for alive, 1 for dead.\n",
    " - `days_to_death`: If the patient has a death event in the `vital_status` column, this is the number of days the patient survived from diagonsis. Otherwise the value is 0 if the patient is still alive.\n",
    " - `days_to_last_followup`: If the patient is still alive, this the last time the patient was known to have followed up and was still alive. Otherwise the value is 0 if the patient is dead.\n",
    " - `overall_survival`: This is either the days until the patient's death or until their last follow-up. This is the max of the aforementioned previous 2 columns.\n",
    "\n",
    "The following is an example of a few lines of the Hofree OV survival table:  \n",
    "\n",
    "||vital_status|days_to_death|days_to_last_followup|overall_survival|\n",
    "|-|-|-|-|-|\n",
    "|TCGA-3P-A9WA|0|0|420|420|\n",
    "|TCGA-59-A5PD|1|624|0|624|\n",
    "|TCGA-5X-AA5U|0|0|361|361|\n",
    "|TCGA-04-1331|1|1336|0|1336|\n",
    "|TCGA-04-1332|1|1247|0|1247|\n",
    "\n",
    "Note: The default setting for pyNBS is that no survival curves are drawn because the survival data is not a required parameter. The path to valid survival data must be explicitly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load survival Data\n",
    "surv_data = './Example_Notebook_Data/Clinical_Files/OV.clin.merged.Hofree.txt'\n",
    "# Plot KM Plot for patient clusters\n",
    "cluster_KMplot(NBS_cluster_assign, surv_data, params=pyNBS_params)\n",
    "Image(filename = pyNBS_params['outdir']+pyNBS_params['job_name']+'_KM_plot.png', width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyNBS Result comparison to Hofree et al 2013\n",
    "We also compare the pyNBS clustering results against the original Hofree 2013 cluster assignments of the same patient data using two scores: adjusted rand index and adjusted mutual information score.\n",
    "  \n",
    "__Adjusted Rand Index (ARI)__\n",
    "> Rand index adjusted for chance.\n",
    "$$ARI = \\frac{RI - \\mathbb{E}(RI)}{ \\max(RI) - \\mathbb{E}(RI)}$$\n",
    "\n",
    "__Adjusted Mutual Info Score (AMI)__\n",
    "> Adjusted Mutual Information between two clusterings.\n",
    "$${AMI(U, V) = \\frac{MI(U, V) - \\mathbb{E}(MI(U, V))}{\\max(H(U), H(V)) - \\mathbb{E}(MI(U, V))}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_mutual_info_score, adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adj_rand_index = adjusted_rand_score(Hofree_OV_clust, pyNBS_OV_clust)\n",
    "adj_mutual_info_score = adjusted_mutual_info_score(Hofree_OV_clust, pyNBS_OV_clust)\n",
    "print 'Adjusted Rand Index is: ' + str(adj_rand_index)\n",
    "print 'Adjusted Mutual Info Score is: ' + str(adj_mutual_info_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
